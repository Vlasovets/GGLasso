{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.pardir)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import itertools\n",
    "from itertools import product\n",
    "\n",
    "from gglasso.solver.single_admm_solver import ADMM_SGL\n",
    "from gglasso.solver.single_admm_solver import block_SGL\n",
    "from gglasso.helper.data_generation import time_varying_power_network, group_power_network, sample_covariance_matrix\n",
    "from gglasso.helper.model_selection import single_grid_search\n",
    "from benchmarks import sklearn_time_benchmark, admm_time_benchmark, model_solution, benchmark_parameters, regain_time_benchmark\n",
    "from benchmarks import time_benchmark, sparsity_benchmark\n",
    "from benchmarks import sk_scaling, single_scaling, block_scaling\n",
    "\n",
    "from plots import plot_accuracy, plot_scalability, plot_lambdas\n",
    "from utils import network_generation, dict_shape, hamming_dict, regain_models_dict, sk_models_dict\n",
    "from utils import benchmarks_dataframe, best_time_dataframe, drop_acc_duplicates\n",
    "\n",
    "from regain.covariance import GraphicalLasso as rg_GL\n",
    "from regain import datasets, utils\n",
    "\n",
    "from sklearn.covariance import GraphicalLasso as sk_GL\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(print_changed_only=False)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-cleaning",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-straight",
   "metadata": {},
   "source": [
    "### Power networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_dict=dict()\n",
    "X_dict=dict()\n",
    "Theta_dict=dict()\n",
    "\n",
    "# p_list=[100, 500, 1000, 2500, 5000, 10000]\n",
    "# N_list=[200, 1000, 2000, 5000, 10000, 20000]\n",
    "p_list=[100, 200]\n",
    "N_list=[200, 400]\n",
    "\n",
    "print(\" Power network generation \".center(40, '-'))\n",
    "\n",
    "for p, N in zip(p_list, N_list):\n",
    "    try:\n",
    "        start = time.perf_counter()\n",
    "        S, X, Theta = network_generation(p, N, K=1, M=2)\n",
    "        end = time.perf_counter()\n",
    "        print(\"p: %5d, N : %5d, Time : %5.4f\" % (p, N, end-start))\n",
    "    except:\n",
    "        print(\"Power network cannot be generated\")\n",
    "        print(\"Tip: increase the number of sub-blocks M\")\n",
    "        break\n",
    "\n",
    "    S_dict[p, N] = S\n",
    "    X_dict[p, N] = X\n",
    "    Theta_dict[p, N] = Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Shape of S_i:\", dict_shape(S_dict))\n",
    "print(\"\\n Shape of X_i:\", dict_shape(X_dict))\n",
    "print(\"\\n Shape of Theta_i:\", dict_shape(Theta_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-inspector",
   "metadata": {},
   "source": [
    "### Latent variables data from <em>regain</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "n_times = [20, 50, 100]\n",
    "n_dims = np.sqrt(np.logspace(2, 5, 10)).astype(int)\n",
    "\n",
    "n_samples = 200\n",
    "n_dim_lat = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "with utils.suppress_stdout():\n",
    "    data = {\n",
    "        (dim, T): datasets.make_dataset(\n",
    "            mode='ma', n_samples=n_samples, \n",
    "            n_dim_lat=n_dim_lat, n_dim_obs=dim,\n",
    "            T=T, epsilon=1e-2)\n",
    "        for dim, T in (product(n_dims, n_times))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lat = dict()\n",
    "\n",
    "for key in data.keys():\n",
    "    X_lat.update({key:data[key]['data']})\n",
    "    \n",
    "X_lat = list(X_lat.values())\n",
    "X_lat = [x for data in X_lat for x in data] #flatten data array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_lat = list() # empirical covariance matrices\n",
    "for i in X_lat:\n",
    "    S_lat.append(np.cov(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-office",
   "metadata": {},
   "source": [
    "### Real soil data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = pd.read_table('/Users/oleg.vlasovetc/Public/GGLasso/data/soil/88soils_rounded_metadata.txt', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-shoulder",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_params, rg_params, admm_params = benchmark_parameters(S_dict=S_dict, sk_tol_list=[0.5], enet_list=[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_list = [0.5, 0.1, 0.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-lithuania",
   "metadata": {},
   "source": [
    "### Model solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_time_dict = dict()\n",
    "model_Z_dict = dict()\n",
    "\n",
    "for X, l1 in itertools.product(list(X_dict.values()), lambda_list):\n",
    "    model = \"sklearn\"\n",
    "    Z, Z_time, info = model_solution(model=model, X=X, lambda1=l1)\n",
    "    \n",
    "    key = \"p_\" + str(X.shape[1]) + \"_l1_\" + str(l1)\n",
    "    model_time_dict.update({key: Z_time})\n",
    "    model_Z_dict.update({key: Z})\n",
    "print(\"Model solution({0}): {1}\".format(model,info))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-elimination",
   "metadata": {},
   "source": [
    "### ADMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dict = dict()\n",
    "accuracy_dict = dict()\n",
    "Z_dict = dict()\n",
    "trace_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, S in zip(list(X_dict.values()), list(S_dict.values())):\n",
    "    Omega_0 = np.eye(len(S))\n",
    "    admm_time, admm_accuracy, Z_admm = admm_time_benchmark(S=S, Omega_0=Omega_0, Z=model_Z_dict, \n",
    "                                                           lambda_list=lambda_list, n_iter=1 + 1,\n",
    "                                                           admm_params=admm_params)\n",
    "    time_dict.update(admm_time)\n",
    "    accuracy_dict.update(admm_accuracy)\n",
    "    Z_dict.update(Z_admm)\n",
    "\n",
    "    for key, item in Z_admm.items():\n",
    "        trace_dict.update({key: {\"Z\": item, \"X\": X, \"S\": S,\"l1\": l1}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-right",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-provider",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sk_models = sk_models_dict(lambda_list = lambda_list, sk_params = sk_params)\n",
    "\n",
    "for X, S in zip(list(X_dict.values()), list(S_dict.values())):\n",
    "    sk_time, sk_accuracy, Z_sk = sklearn_time_benchmark(sk_models, X=X, Z=model_Z_dict, n_iter=2)\n",
    "    \n",
    "    time_dict.update(sk_time)\n",
    "    accuracy_dict.update(sk_accuracy)\n",
    "    Z_dict.update(Z_sk)\n",
    "    \n",
    "    for key, item in Z_sk.items():\n",
    "        trace_dict.update({key: {\"Z\": item, \"X\": X, \"S\": S,\"l1\": l1}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-assets",
   "metadata": {},
   "source": [
    "### Regain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-gravity",
   "metadata": {},
   "outputs": [],
   "source": [
    "regain_models = regain_models_dict(lambda_list = lambda_list, rg_params = rg_params)\n",
    "\n",
    "for X, S in zip(list(X_dict.values()), list(S_dict.values())):\n",
    "    rg_time, rg_accuracy, Z_rg = regain_time_benchmark(regain_models, X=X, Z=model_Z_dict, n_iter=2)\n",
    "    \n",
    "    time_dict.update(rg_time)\n",
    "    accuracy_dict.update(rg_accuracy)\n",
    "    Z_dict.update(Z_rg)\n",
    "    \n",
    "    for key, item in Z_rg.items():\n",
    "        trace_dict.update({key: {\"Z\": item, \"X\": X, \"S\": S,\"l1\": l1}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-title",
   "metadata": {},
   "source": [
    "### Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-hello",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity = hamming_dict(Theta_dict=Theta_dict, Z_dict=Z_dict, t_rounding=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-weather",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = benchmarks_dataframe(times=time_dict, acc_dict=accuracy_dict, spars_dict=sparsity)\n",
    "df = drop_acc_duplicates(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-consciousness",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_lambdas(df, upper_bound=0.01, lower_bound=0.0001)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_accuracy(df, upper_bound=0.01, lower_bound=0.0001)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = sparsity_benchmark(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames[1000]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
